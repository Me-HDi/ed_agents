{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF2 PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from mistralai import Mistral\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "\n",
    "#openai = OpenAI()\n",
    "mistralai_api_key = os.getenv('MISTRAL_API_KEY')\n",
    "mistralai = Mistral(api_key= mistralai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/Mehdi, EL HAYLALI-CV.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Mehdi EL HAYLALI \n",
      "  Data Scientist  \n",
      " \n",
      "  \n",
      "Phone \n",
      " \n",
      "+212 708581004 \n",
      "Email \n",
      " \n",
      "mehdi.el.haylali@gmail.com \n",
      "Location \n",
      " \n",
      "Morocco \n",
      " \n",
      "LinkedIn \n",
      " \n",
      "www.linkedin.com/in/mehdi-el-haylali \n",
      " \n",
      " Profile \n",
      "Senior data scientist with 4 years of consulting experience in the Analytics & AI field. My proficiency in \n",
      "Machine Learning, MLOps and Data engineering enables me to develop and deploy scalable solutions that \n",
      "meet business needs. I efficiently collaborate with cross-functional teams to integrate data-driven insights \n",
      "into business decision-making. Example projects include developing a recommendation system for a retail \n",
      "business, predicting customers upsell as well as predicting RFPs ’ outcomes. In addition to my traditional \n",
      "ML expertise, I earned practical skills in developing LLM -based solutions using langchain framework and \n",
      "frontier models through hands-on trainings. \n",
      " Key Skills \n",
      "o Data science, MLOps, Advanced Analytics, Data Engineering, Generative AI (OpenAI, MistralAI, Prompt \n",
      "Engineering) \n",
      "o Python, R, Spark, SQL, DBT, Git, Docker, scikit-learn, keras, pytorch, CatBoost, LightGBM, MLflow \n",
      "o GCP, AWS, Azure, Databricks, watsonx.ai \n",
      "o Analytical thinking, Problem solving, Efficient communication \n",
      "Work Experience  \n",
      "Data Scientist/Engineer \n",
      "IBM Consulting, Morocco April 2021–present \n",
      " \n",
      "Client: IBM Consulting - internal \n",
      "Project name: RFP’s outcome (Win/Loss) prediction \n",
      "Objective: Developed, deployed and automated an end-to-end machine learning solution that \n",
      "generates an RFP’s win probability and enables Offering Managers to tailor their RFP responses for \n",
      "higher win probability. \n",
      "Achievements: \n",
      "o Analyzed IBM signings data using Spark, Python and SQL, processing 600K+ rows of data from  a \n",
      "PostgreSQL RDBS. \n",
      "o Selected and engineered RFPs’ features like offering name, customer segmentation, opportunity \n",
      "owner using scikit-learn. \n",
      "o Built and experimented with many classification models (Logistic Regression, Random Forest, \n",
      "Gradient Boosting algorithms, ANN) using scikit-learn, keras, LightGBM, CatBoost, XGBoost. \n",
      "o Performed hyperparameters-tuning on CatBoost model using HyperOpt resulting in +80% accuracy. \n",
      "o Deployed the trained CatBoost model in production as online endpoint for real-time predictions. \n",
      "o Deployed the trained CatBoost model in production for scheduled batch predictions, scoring 60K+ \n",
      "open deal each week. \n",
      "o Integrated model’s batch predictions into the PostgreSQL RDBS to feed BI tools (tableau). \n",
      "o Built automated spark jobs for data preparation. \n",
      "o Built automated python jobs to monitor the model’s performance on a weekly -basis and trigger \n",
      "model retraining in case of performance drop. \n",
      "o Integrated shapley values to explain individual predictions and hence make the model interpretable \n",
      "and trustworthy. \n",
      "o Developed a creative solution that suggests what changes to make to an RFP’s features to increase \n",
      "its win probability. \n",
      "o Currently working on data migration from Postgres to Db2 and watsonx.data.  Tech stack: IBM Cloud, watsonx.ai, watsonx.data, Python, Spark, SQL, Postgres, Db2, Presto engine, \n",
      "scikit-learn, Keras, CatBoost, HyperOpt, LightGBM, XGBoost, SHAP. \n",
      "Client: Nespresso USA \n",
      "Project name: Customer upsell prediction \n",
      "Objective: Developed a predictive model that estimates customers’ upselling likelihood. This allows \n",
      "the business to focus their marketing efforts on customers with high potential. \n",
      "Achievements: \n",
      "o Analyzed Nespresso US sales data for capsules, coffee machines and accessories using spark on \n",
      "databricks. \n",
      "o Labelled customers based on upsell occurrence. \n",
      "o Engineered features like purchase frequency, total expenditures, coffee preferences and other \n",
      "Nespresso products consumption behaviours. \n",
      "o Built the end-to-end data preparation spark job. \n",
      "o Built and experimented with many classification models. \n",
      "o Enabled machine learning experiments tracking besides seamless models’ management and \n",
      "serving using MLflow. \n",
      "o Performed hyper-parameters tuning for LightGBM model to achieve optimal performance in terms \n",
      "of cumulative gains and lift. \n",
      "o Deployed the trained model for on-demand batch predictions. \n",
      "o Generated model explainability charts like Dependence Plots and Force Plots to derive insights on \n",
      "features impacts. \n",
      "Tech stack: Azure Databricks, Delta Lake, Spark, SQL, MLflow, HyperOpt, scikit-learn, CatBoost \n",
      "LightGBM, Keras, SHAP, CRISP-DM methodology. \n",
      "Client: Nespresso USA \n",
      "Project name: Content-based recommendation system \n",
      "Objective: Built a content-based recommendation system to provide personalised coffee \n",
      "recommendations to Nespresso customers. \n",
      "Achievements: \n",
      "o Gathered attributes that describe each Nespresso coffee capsule product like intensity, acidity, \n",
      "roastiness, flavor, aroma..etc. \n",
      "o Vectorized the collected coffee attributes using scikit-learn. \n",
      "o Collected and ranked each customer’s coffee preferences based on their purchase history. \n",
      "o For each customer, compute the cosine similarity between its preferred coffees and other coffee  \n",
      "items. \n",
      "o Recommend the top-N similar coffee items for the customer. \n",
      "o Evaluated model performance using Mean Average Precision & Coverage. \n",
      "o Enabled experiment tracking using MLflow. \n",
      "Tech stack: Azure Databricks, Delta Lake, Spark, SQL, MLflow, scikit-learn, CRISP-DM methodology. \n",
      "Client: DS Smith UK \n",
      "Project name: Production capacity planning app \n",
      "Objective: Build a serverless web application to facilitate production capacity planning \n",
      "for supply chain managers. \n",
      "Achievements: \n",
      "o Built the end-to-end data ingestion pipeline. \n",
      "o Designed and implemented the data model for the application in AWS Redshift. \n",
      "o Translated complex business logic into concrete python code and SQL queries and packaged that \n",
      "as Lambda Functions that get triggered by the user's actions in the frontend. \n",
      "o Requirements and knowledge gathering from the business stakeholders. Tech stack: Python, SQL, AWS Glue, AWS Lambda, AWS Redshift & S3. \n",
      "Client: Newmont Australia \n",
      "Project name: Advanced analytics for mining operations \n",
      "Objective: Delivered data products and solutions to support mining operations in Boddington-\n",
      "Australia. \n",
      "Achievements: \n",
      "o Batch and real-time data ingestion and parsing from various data sources using Google Cloud \n",
      "Functions, Cloud Run, Python and Docker. \n",
      "o Designed and implemented advanced SQL queries to answer the business’ analytics needs using \n",
      "BigQuery, DBT and jinja. \n",
      "Tech stack: Python (Pycharm IDE), GCP (GCS, Cloud Functions, Pub/Sub, Cloud Scheduler, Cloud \n",
      "Run, BigQuery), Docker, SQL, DBT, git. \n",
      "Client: General Motors \n",
      "Project name: Data science projects migration \n",
      "Objective: Migrated two data science projects from SAS to Pyspark and SparklyR. \n",
      "Achievements: \n",
      "o SAS code migration to PySpark and SparklyR (ETL + time series analysis and forecasting codes). \n",
      "o Optimization of the Spark jobs' execution time by tuning cluster configuration parameters and  \n",
      "implementing caching strategies. \n",
      "o Productionization of python and R scripts.  \n",
      "Tech stack: Python & R (JupyterLab), Apache Spark & Hive, statsmodels. \n",
      "Data Science Intern \n",
      "Saint-Gobain Crystals, France Mar 2020–Aug 2020 \n",
      " \n",
      "o Created an interactive dashboard to help the customer service department monitor production \n",
      "lead-times and delivery delays. \n",
      "o Investigated the root cause for material consumption imbalance using data analysis techniques. \n",
      "o Established a mathematical model to estimate and optimize the production capacity. \n",
      "Tech stack: Python (Pandas, Bokeh, Holoview, Panel), Data analysis and visualization. \n",
      " \n",
      "Education \n",
      "Ecole Centrale Casablanca, Morocco September 2017–October 2020 \n",
      "Master of engineering — Data science \n",
      "Courses: Big Data technologies, Statistical Learning, Applications for Deep Learning, High-dimensional \n",
      "Statistics and Massive Data, Optimization. \n",
      " \n",
      "Preparatory classes, Morocco September 2015– October 2017 \n",
      "Mathematics, physics & engineering (MPSI-MP*) \n",
      "Certifications \n",
      "o Watsonx.ai Technical Sales Advanced – 2024  \n",
      "o IBM Consulting Way Habits - 2024 \n",
      "o Machine Learning Engineering for Production (MLOps) – coursera – 2023 \n",
      "o Databricks Certified Machine Learning Professional – 2023  o Databricks Certified Machine Learning Associate – 2023 \n",
      "o AWS Certified Machine Learning – Specialty – 2022 \n",
      "o Microsoft Certified: Azure Data Scientist Associate – 2021 \n",
      "Languages \n",
      "English - fluent | French – fluent | Arabic – native \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Mehdi EL HAYLALI. I was born in Fes, Morocco. I love coding and reading arabic books. I am currently living and working in Casablanca.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Mehdi EL HAYLALI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are acting as Mehdi EL HAYLALI. You are answering questions on Mehdi EL HAYLALI's website, particularly questions related to Mehdi EL HAYLALI's career, background, skills and experience. Your responsibility is to represent Mehdi EL HAYLALI for interactions on the website as faithfully as possible. You are given a summary of Mehdi EL HAYLALI's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\n",
      "\n",
      "## Summary:\n",
      "My name is Mehdi EL HAYLALI. I was born in Fes, Morocco. I love coding and reading arabic books. I am currently living and working in Casablanca.\n",
      "\n",
      "## LinkedIn Profile:\n",
      " \n",
      "Mehdi EL HAYLALI \n",
      "  Data Scientist  \n",
      " \n",
      "  \n",
      "Phone \n",
      " \n",
      "+212 708581004 \n",
      "Email \n",
      " \n",
      "mehdi.el.haylali@gmail.com \n",
      "Location \n",
      " \n",
      "Morocco \n",
      " \n",
      "LinkedIn \n",
      " \n",
      "www.linkedin.com/in/mehdi-el-haylali \n",
      " \n",
      " Profile \n",
      "Senior data scientist with 4 years of consulting experience in the Analytics & AI field. My proficiency in \n",
      "Machine Learning, MLOps and Data engineering enables me to develop and deploy scalable solutions that \n",
      "meet business needs. I efficiently collaborate with cross-functional teams to integrate data-driven insights \n",
      "into business decision-making. Example projects include developing a recommendation system for a retail \n",
      "business, predicting customers upsell as well as predicting RFPs ’ outcomes. In addition to my traditional \n",
      "ML expertise, I earned practical skills in developing LLM -based solutions using langchain framework and \n",
      "frontier models through hands-on trainings. \n",
      " Key Skills \n",
      "o Data science, MLOps, Advanced Analytics, Data Engineering, Generative AI (OpenAI, MistralAI, Prompt \n",
      "Engineering) \n",
      "o Python, R, Spark, SQL, DBT, Git, Docker, scikit-learn, keras, pytorch, CatBoost, LightGBM, MLflow \n",
      "o GCP, AWS, Azure, Databricks, watsonx.ai \n",
      "o Analytical thinking, Problem solving, Efficient communication \n",
      "Work Experience  \n",
      "Data Scientist/Engineer \n",
      "IBM Consulting, Morocco April 2021–present \n",
      " \n",
      "Client: IBM Consulting - internal \n",
      "Project name: RFP’s outcome (Win/Loss) prediction \n",
      "Objective: Developed, deployed and automated an end-to-end machine learning solution that \n",
      "generates an RFP’s win probability and enables Offering Managers to tailor their RFP responses for \n",
      "higher win probability. \n",
      "Achievements: \n",
      "o Analyzed IBM signings data using Spark, Python and SQL, processing 600K+ rows of data from  a \n",
      "PostgreSQL RDBS. \n",
      "o Selected and engineered RFPs’ features like offering name, customer segmentation, opportunity \n",
      "owner using scikit-learn. \n",
      "o Built and experimented with many classification models (Logistic Regression, Random Forest, \n",
      "Gradient Boosting algorithms, ANN) using scikit-learn, keras, LightGBM, CatBoost, XGBoost. \n",
      "o Performed hyperparameters-tuning on CatBoost model using HyperOpt resulting in +80% accuracy. \n",
      "o Deployed the trained CatBoost model in production as online endpoint for real-time predictions. \n",
      "o Deployed the trained CatBoost model in production for scheduled batch predictions, scoring 60K+ \n",
      "open deal each week. \n",
      "o Integrated model’s batch predictions into the PostgreSQL RDBS to feed BI tools (tableau). \n",
      "o Built automated spark jobs for data preparation. \n",
      "o Built automated python jobs to monitor the model’s performance on a weekly -basis and trigger \n",
      "model retraining in case of performance drop. \n",
      "o Integrated shapley values to explain individual predictions and hence make the model interpretable \n",
      "and trustworthy. \n",
      "o Developed a creative solution that suggests what changes to make to an RFP’s features to increase \n",
      "its win probability. \n",
      "o Currently working on data migration from Postgres to Db2 and watsonx.data.  Tech stack: IBM Cloud, watsonx.ai, watsonx.data, Python, Spark, SQL, Postgres, Db2, Presto engine, \n",
      "scikit-learn, Keras, CatBoost, HyperOpt, LightGBM, XGBoost, SHAP. \n",
      "Client: Nespresso USA \n",
      "Project name: Customer upsell prediction \n",
      "Objective: Developed a predictive model that estimates customers’ upselling likelihood. This allows \n",
      "the business to focus their marketing efforts on customers with high potential. \n",
      "Achievements: \n",
      "o Analyzed Nespresso US sales data for capsules, coffee machines and accessories using spark on \n",
      "databricks. \n",
      "o Labelled customers based on upsell occurrence. \n",
      "o Engineered features like purchase frequency, total expenditures, coffee preferences and other \n",
      "Nespresso products consumption behaviours. \n",
      "o Built the end-to-end data preparation spark job. \n",
      "o Built and experimented with many classification models. \n",
      "o Enabled machine learning experiments tracking besides seamless models’ management and \n",
      "serving using MLflow. \n",
      "o Performed hyper-parameters tuning for LightGBM model to achieve optimal performance in terms \n",
      "of cumulative gains and lift. \n",
      "o Deployed the trained model for on-demand batch predictions. \n",
      "o Generated model explainability charts like Dependence Plots and Force Plots to derive insights on \n",
      "features impacts. \n",
      "Tech stack: Azure Databricks, Delta Lake, Spark, SQL, MLflow, HyperOpt, scikit-learn, CatBoost \n",
      "LightGBM, Keras, SHAP, CRISP-DM methodology. \n",
      "Client: Nespresso USA \n",
      "Project name: Content-based recommendation system \n",
      "Objective: Built a content-based recommendation system to provide personalised coffee \n",
      "recommendations to Nespresso customers. \n",
      "Achievements: \n",
      "o Gathered attributes that describe each Nespresso coffee capsule product like intensity, acidity, \n",
      "roastiness, flavor, aroma..etc. \n",
      "o Vectorized the collected coffee attributes using scikit-learn. \n",
      "o Collected and ranked each customer’s coffee preferences based on their purchase history. \n",
      "o For each customer, compute the cosine similarity between its preferred coffees and other coffee  \n",
      "items. \n",
      "o Recommend the top-N similar coffee items for the customer. \n",
      "o Evaluated model performance using Mean Average Precision & Coverage. \n",
      "o Enabled experiment tracking using MLflow. \n",
      "Tech stack: Azure Databricks, Delta Lake, Spark, SQL, MLflow, scikit-learn, CRISP-DM methodology. \n",
      "Client: DS Smith UK \n",
      "Project name: Production capacity planning app \n",
      "Objective: Build a serverless web application to facilitate production capacity planning \n",
      "for supply chain managers. \n",
      "Achievements: \n",
      "o Built the end-to-end data ingestion pipeline. \n",
      "o Designed and implemented the data model for the application in AWS Redshift. \n",
      "o Translated complex business logic into concrete python code and SQL queries and packaged that \n",
      "as Lambda Functions that get triggered by the user's actions in the frontend. \n",
      "o Requirements and knowledge gathering from the business stakeholders. Tech stack: Python, SQL, AWS Glue, AWS Lambda, AWS Redshift & S3. \n",
      "Client: Newmont Australia \n",
      "Project name: Advanced analytics for mining operations \n",
      "Objective: Delivered data products and solutions to support mining operations in Boddington-\n",
      "Australia. \n",
      "Achievements: \n",
      "o Batch and real-time data ingestion and parsing from various data sources using Google Cloud \n",
      "Functions, Cloud Run, Python and Docker. \n",
      "o Designed and implemented advanced SQL queries to answer the business’ analytics needs using \n",
      "BigQuery, DBT and jinja. \n",
      "Tech stack: Python (Pycharm IDE), GCP (GCS, Cloud Functions, Pub/Sub, Cloud Scheduler, Cloud \n",
      "Run, BigQuery), Docker, SQL, DBT, git. \n",
      "Client: General Motors \n",
      "Project name: Data science projects migration \n",
      "Objective: Migrated two data science projects from SAS to Pyspark and SparklyR. \n",
      "Achievements: \n",
      "o SAS code migration to PySpark and SparklyR (ETL + time series analysis and forecasting codes). \n",
      "o Optimization of the Spark jobs' execution time by tuning cluster configuration parameters and  \n",
      "implementing caching strategies. \n",
      "o Productionization of python and R scripts.  \n",
      "Tech stack: Python & R (JupyterLab), Apache Spark & Hive, statsmodels. \n",
      "Data Science Intern \n",
      "Saint-Gobain Crystals, France Mar 2020–Aug 2020 \n",
      " \n",
      "o Created an interactive dashboard to help the customer service department monitor production \n",
      "lead-times and delivery delays. \n",
      "o Investigated the root cause for material consumption imbalance using data analysis techniques. \n",
      "o Established a mathematical model to estimate and optimize the production capacity. \n",
      "Tech stack: Python (Pandas, Bokeh, Holoview, Panel), Data analysis and visualization. \n",
      " \n",
      "Education \n",
      "Ecole Centrale Casablanca, Morocco September 2017–October 2020 \n",
      "Master of engineering — Data science \n",
      "Courses: Big Data technologies, Statistical Learning, Applications for Deep Learning, High-dimensional \n",
      "Statistics and Massive Data, Optimization. \n",
      " \n",
      "Preparatory classes, Morocco September 2015– October 2017 \n",
      "Mathematics, physics & engineering (MPSI-MP*) \n",
      "Certifications \n",
      "o Watsonx.ai Technical Sales Advanced – 2024  \n",
      "o IBM Consulting Way Habits - 2024 \n",
      "o Machine Learning Engineering for Production (MLOps) – coursera – 2023 \n",
      "o Databricks Certified Machine Learning Professional – 2023  o Databricks Certified Machine Learning Associate – 2023 \n",
      "o AWS Certified Machine Learning – Specialty – 2022 \n",
      "o Microsoft Certified: Azure Data Scientist Associate – 2021 \n",
      "Languages \n",
      "English - fluent | French – fluent | Arabic – native \n",
      " \n",
      " \n",
      "\n",
      "With this context, please chat with the user, always staying in character as Mehdi EL HAYLALI.\n"
     ]
    }
   ],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"mistral-large-latest\"\n",
    "\n",
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = mistralai.chat.complete(model=\"mistral-large-latest\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += f\"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you hold a patent?\"}]\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(reply, \"do you hold a patent?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + f\"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"patent\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
